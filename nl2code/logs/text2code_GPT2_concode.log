[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(data_dir='data/concode', langs='java', output_dir='save/concode', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', config_dir=None, tokenizer_dir=None, load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, do_infer=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=2, learning_rate=5e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=5000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=-1, server_ip='', server_port='', log_file='logs/text2code_GPT2_concode.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Creating features from dataset file at data/concode\train.json
Data size: 100000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0 Training 100000 token, 100000 samples
Saving features into cached file save/concode\train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 1
  Instantaneous batch size per GPU = 8
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 2
  Total optimization steps = 6250
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(data_dir='data/concode', langs='java', output_dir='save/concode', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', config_dir=None, tokenizer_dir=None, load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, do_infer=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=6, per_gpu_eval_batch_size=6, gradient_accumulation_steps=2, learning_rate=5e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=5000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=-1, server_ip='', server_port='', log_file='logs/text2code_GPT2_concode.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/concode\train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 0
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 8333
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(data_dir='data/concode', langs='java', output_dir='save/concode', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', config_dir=None, tokenizer_dir=None, load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=64, do_train=True, do_eval=False, do_infer=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=6, per_gpu_eval_batch_size=6, gradient_accumulation_steps=2, learning_rate=5e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=5000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=-1, server_ip='', server_port='', log_file='logs/text2code_GPT2_concode.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Creating features from dataset file at data/concode\train.json
Data size: 100000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0 Training 100000 token, 100000 samples
Saving features into cached file save/concode\train_blocksize_64_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 0
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 8333
  steps: 100  ppl: 5.1693
  steps: 200  ppl: 3.1368
  steps: 300  ppl: 2.9676
  steps: 400  ppl: 2.8324
  steps: 500  ppl: 2.6718
  steps: 600  ppl: 2.7015
  steps: 700  ppl: 2.4838
  steps: 800  ppl: 2.3956
  steps: 900  ppl: 2.4441
  steps: 1000  ppl: 2.3959
  steps: 1100  ppl: 2.3832
  steps: 1200  ppl: 2.3437
  steps: 1300  ppl: 2.2679
  steps: 1400  ppl: 2.213
  steps: 1500  ppl: 2.3235
  steps: 1600  ppl: 2.2083
  steps: 1700  ppl: 2.2424
  steps: 1800  ppl: 2.187
  steps: 1900  ppl: 2.2251
  steps: 2000  ppl: 2.2075
  steps: 2100  ppl: 2.1366
  steps: 2200  ppl: 2.2053
  steps: 2300  ppl: 2.1575
  steps: 2400  ppl: 2.1415
  steps: 2500  ppl: 2.0931
  steps: 2600  ppl: 2.1689
  steps: 2700  ppl: 2.0828
  steps: 2800  ppl: 2.1295
  steps: 2900  ppl: 2.0455
  steps: 3000  ppl: 2.1458
  steps: 3100  ppl: 2.1352
  steps: 3200  ppl: 2.1067
  steps: 3300  ppl: 2.1088
  steps: 3400  ppl: 2.104
  steps: 3500  ppl: 2.1002
  steps: 3600  ppl: 2.0764
  steps: 3700  ppl: 2.089
  steps: 3800  ppl: 2.0048
  steps: 3900  ppl: 2.0795
  steps: 4000  ppl: 2.0293
  steps: 4100  ppl: 2.093
  steps: 4200  ppl: 2.0823
  steps: 4300  ppl: 2.0251
  steps: 4400  ppl: 2.0414
  steps: 4500  ppl: 2.1497
  steps: 4600  ppl: 2.0402
  steps: 4700  ppl: 2.0035
  steps: 4800  ppl: 2.0057
  steps: 4900  ppl: 2.0176
  steps: 5000  ppl: 2.0339
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
  perplexity = 2.4787
Saving model checkpoint to save/concode\checkpoint-5000-2.4787
Saving optimizer and scheduler states to save/concode\checkpoint-last
  steps: 5100  ppl: 2.0315
  steps: 5200  ppl: 2.0385
  steps: 5300  ppl: 1.9798
  steps: 5400  ppl: 1.9549
  steps: 5500  ppl: 1.988
  steps: 5600  ppl: 1.9576
reload model from save/concode\checkpoint-last, resume from 1 epoch
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "save/concode\\checkpoint-last",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(data_dir='data/concode', langs='java', output_dir='save/concode', model_type='gpt2', pretrain_dir='save/concode\\checkpoint-last', config_dir=None, tokenizer_dir=None, load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=64, do_train=True, do_eval=False, do_infer=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=6, per_gpu_eval_batch_size=6, gradient_accumulation_steps=2, learning_rate=5e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=5000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=-1, server_ip='', server_port='', log_file='logs/text2code_GPT2_concode.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=1, start_step=5000, config_name='save/concode\\checkpoint-last\\config.json')
Loading features from cached file save/concode\train_blocksize_64_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 0
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 8333
 global_step = 5000, average loss = 0.0
reload model from save/concode\checkpoint-last, resume from 1 epoch
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "save/concode\\checkpoint-last",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(data_dir='data/concode', langs='java', output_dir='save/concode', model_type='gpt2', pretrain_dir='save/concode\\checkpoint-last', config_dir=None, tokenizer_dir=None, load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=64, do_train=True, do_eval=False, do_infer=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=6, per_gpu_eval_batch_size=6, gradient_accumulation_steps=2, learning_rate=5e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=5000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=-1, server_ip='', server_port='', log_file='logs/text2code_GPT2_concode.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=1, start_step=5000, config_name='save/concode\\checkpoint-last\\config.json')
Loading features from cached file save/concode\train_blocksize_64_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 0
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 8333
 global_step = 5000, average loss = 0.0
reload model from save/concode\checkpoint-last, resume from 1 epoch
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "save/concode\\checkpoint-last",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(data_dir='data/concode', langs='java', output_dir='save/concode', model_type='gpt2', pretrain_dir='save/concode\\checkpoint-last', config_dir=None, tokenizer_dir=None, load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=64, do_train=True, do_eval=False, do_infer=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=6, per_gpu_eval_batch_size=6, gradient_accumulation_steps=2, learning_rate=5e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=5000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=-1, server_ip='', server_port='', log_file='logs/text2code_GPT2_concode.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=1, start_step=5000, config_name='save/concode\\checkpoint-last\\config.json')
Loading features from cached file save/concode\train_blocksize_64_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 0
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 8333
 global_step = 5000, average loss = 0.0
[50259, 23748, 995, 50257, 50258]
GPT2Config {
  "_name_or_path": "microsoft/CodeGPT-small-java-adaptedGPT2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50259,
  "embd_pdrop": 0.1,
  "eos_token_id": 50258,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "pad_token_id": 50257,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.17.0",
  "use_cache": true,
  "vocab_size": 50261
}

Model has a total of 124442880 trainable parameters
Training/evaluation parameters Namespace(data_dir='data/concode', langs='java', output_dir='save/concode', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-java-adaptedGPT2', config_dir=None, tokenizer_dir=None, load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=64, do_train=True, do_eval=False, do_infer=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=6, per_gpu_eval_batch_size=6, gradient_accumulation_steps=2, learning_rate=5e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=5000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=-1, server_ip='', server_port='', log_file='logs/text2code_GPT2_concode.log', tensorboard_dir=None, n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Creating features from dataset file at data/concode\train.json
Data size: 100000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0 Training 100000 token, 100000 samples
Saving features into cached file save/concode\train_blocksize_64_wordsize_1_rank_0
***** Running training *****
  Num examples = 100000
  Num epoch = 0
  Instantaneous batch size per GPU = 6
  Total train batch size (w. parallel, distributed & accumulation) = 12
  Gradient Accumulation steps = 2
  Total optimization steps = 8333
  steps: 100  ppl: 5.1693
  steps: 200  ppl: 3.1368
  steps: 300  ppl: 2.9676
  steps: 400  ppl: 2.8324
  steps: 500  ppl: 2.6718
  steps: 600  ppl: 2.7015
  steps: 700  ppl: 2.4838
  steps: 800  ppl: 2.3956
  steps: 900  ppl: 2.4441
  steps: 1000  ppl: 2.3959
  steps: 1100  ppl: 2.3832
  steps: 1200  ppl: 2.3437
  steps: 1300  ppl: 2.2679
  steps: 1400  ppl: 2.213
  steps: 1500  ppl: 2.3235
  steps: 1600  ppl: 2.2083
  steps: 1700  ppl: 2.2424
  steps: 1800  ppl: 2.187
  steps: 1900  ppl: 2.2251
  steps: 2000  ppl: 2.2075
  steps: 2100  ppl: 2.1366
  steps: 2200  ppl: 2.2053
  steps: 2300  ppl: 2.1575
  steps: 2400  ppl: 2.1415
  steps: 2500  ppl: 2.0931
  steps: 2600  ppl: 2.1689
  steps: 2700  ppl: 2.0828
  steps: 2800  ppl: 2.1295
  steps: 2900  ppl: 2.0455
  steps: 3000  ppl: 2.1458
  steps: 3100  ppl: 2.1352
  steps: 3200  ppl: 2.1067
  steps: 3300  ppl: 2.1088
  steps: 3400  ppl: 2.104
  steps: 3500  ppl: 2.1002
  steps: 3600  ppl: 2.0764
  steps: 3700  ppl: 2.089
  steps: 3800  ppl: 2.0048
  steps: 3900  ppl: 2.0795
  steps: 4000  ppl: 2.0293
  steps: 4100  ppl: 2.093
  steps: 4200  ppl: 2.0823
  steps: 4300  ppl: 2.0251
  steps: 4400  ppl: 2.0414
  steps: 4500  ppl: 2.1497
  steps: 4600  ppl: 2.0402
  steps: 4700  ppl: 2.0035
  steps: 4800  ppl: 2.0057
  steps: 4900  ppl: 2.0176
  steps: 5000  ppl: 2.0339
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
  perplexity = 2.4787
Data size: 2000
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
